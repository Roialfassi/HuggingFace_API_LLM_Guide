{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea4f5cd1-4710-4c70-b5cf-28cb88f943d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "api_token = \"Your Token Here\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de7cf426-0177-411d-9ca8-9bc8ac3583d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae5f95f4-608a-4ffc-a665-9bccaafe93f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = \"gpt2\"  # You can replace this with any other model available on Hugging Face\n",
    "\n",
    "# Hugging Face Inference API URL\n",
    "api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "\n",
    "# Headers for authorization\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_token}\"\n",
    "}\n",
    "\n",
    "# Your prompt\n",
    "prompt = \"Once upon a time in a faraway land,\"\n",
    "\n",
    "# Payload to send to the API\n",
    "payload = {\n",
    "    \"inputs\": prompt,\n",
    "    \"parameters\": {\n",
    "        \"max_length\": 50,  # Adjust the max length as needed\n",
    "    }\n",
    "}\n",
    "\n",
    "# Make the request to the API\n",
    "response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "# Print the response\n",
    "if response.status_code == 200:\n",
    "    print(response.json())\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}, {response.text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eeb5e20-eb61-4a16-bb1c-2c20e38b7469",
   "metadata": {},
   "source": [
    "# Parameters:\n",
    "\n",
    "## System Prompt\n",
    "\n",
    "What: A preamble or context-setting input to guide the model's behavior.\n",
    "\n",
    "Why: Helps in defining the role of the assistant, e.g., \"You are a friendly chatbot.\"\n",
    "\n",
    "## User Prompt\n",
    "\n",
    "What: The actual question or task input provided by the user.\n",
    "\n",
    "Why: This is the primary input the model will respond to.\n",
    "\n",
    "## Max Length\n",
    "\n",
    "What: The maximum number of tokens (words or subwords) in the model's response.\n",
    "\n",
    "Why: Limits the length of the response to save costs and focus on concise answers.\n",
    "\n",
    "## Temperature\n",
    "\n",
    "What: Controls the randomness of the model's output.\n",
    "\n",
    "Lower values (e.g., 0.2) = deterministic and focused responses.\n",
    "\n",
    "Higher values (e.g., 0.8) = more creative and diverse responses.\n",
    "\n",
    "\n",
    "## Top-p (Nucleus Sampling)\n",
    "\n",
    "What: Probability threshold for selecting tokens. The model picks from the smallest set of tokens whose probabilities add up to p.\n",
    "\n",
    "Why: Offers better control over randomness without limiting diversity as much as top-k.\n",
    "\n",
    "## Top-k Sampling\n",
    "\n",
    "What: Limits sampling to the k most likely next tokens.\n",
    "\n",
    "Why: Reduces randomness by focusing on the most likely options.\n",
    "\n",
    "## Repetition Penalty\n",
    "\n",
    "What: Penalizes the model for repeating the same phrases.\n",
    "\n",
    "Why: Prevents repetitive or verbose outputs.\n",
    "\n",
    "## Stop Sequences\n",
    "\n",
    "What: Strings or tokens that stop the modelâ€™s response when encountered.\n",
    "\n",
    "Why: Ensures responses end appropriately (e.g., after completing a sentence or phrase).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "961d7cde-97f3-4e2b-8b07-33e2303df721",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HuggingFaceWrapper:\n",
    "    def __init__(self, api_token):\n",
    "        self.api_token = api_token\n",
    "\n",
    "    def query(self, model, prompt, max_length=100, temperature=0.7, top_p=0.9, \n",
    "              top_k=50, repetition_penalty=1.2, stop_sequences=None, debug=False):\n",
    "        api_url = f\"https://api-inference.huggingface.co/models/{model}\"\n",
    "        headers = {\"Authorization\": f\"Bearer {self.api_token}\"}\n",
    "        payload = {\n",
    "            \"inputs\": prompt,\n",
    "            \"parameters\": {\n",
    "                \"max_length\": max_length,\n",
    "                \"temperature\": temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"top_k\": top_k,\n",
    "                \"repetition_penalty\": repetition_penalty,\n",
    "                \"stop_sequences\": stop_sequences,\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        if debug:\n",
    "            print(f\"Payload: {payload}\")\n",
    "        \n",
    "        response = requests.post(api_url, headers=headers, json=payload)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            return response.json()\n",
    "        else:\n",
    "            return {\"error\": response.text}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6003767a-f363-42fe-a3aa-f8c4c9bb5339",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf = HuggingFaceWrapper(api_token)\n",
    "\n",
    "# Query the model with all customizations\n",
    "response = hf.query(\n",
    "    model=\"gpt2\",\n",
    "    prompt=\"Once upon a time in a magical forest,\",\n",
    "    max_length=50,\n",
    "    temperature=0.8,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1,\n",
    "    stop_sequences=[\"\\n\"],\n",
    "    debug=True\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
